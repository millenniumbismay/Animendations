{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast"
      ],
      "metadata": {
        "id": "w_XlKkA5MYQP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animes_data_path = \"preprocessed_animes.csv\"\n",
        "profiles_data_path = \"preprocessed_profiles.csv\"\n",
        "reviews_data_path = \"preprocessed_reviews.csv\"\n",
        "animes_data = pd.read_csv(animes_data_path)\n",
        "profiles_data = pd.read_csv(profiles_data_path)\n",
        "reviews_data = pd.read_csv(reviews_data_path, engine='python', sep=',', error_bad_lines=False)\n",
        "\n",
        "import pickle \n",
        "vit_50_dict = {}\n",
        "with open(\"compressed_vit_50_mapping_embeddings.pickle\", \"rb\") as f:\n",
        "  vit_50_dict = pickle.load(f)\n",
        "q1 = reviews_data['anime_uid'].unique().tolist()\n",
        "q2 = [x for x in vit_50_dict.keys()]\n",
        "s1 = set(q1)\n",
        "s2 = set(q2)\n",
        "s3 = s1-s2 \n",
        "l3 = list(s3)\n",
        "\n",
        "animes = animes_data.copy()\n",
        "profiles = profiles_data.copy()\n",
        "reviews = reviews_data.copy()\n",
        "for x in l3:\n",
        "  reviews.drop(reviews[reviews['anime_uid'] == x].index, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-U_xuwqLm2s",
        "outputId": "a0e6e85e-da65-46dc-9752-8530f4ad9c1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ea4d700ddb98>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  reviews_data = pd.read_csv(reviews_data_path, engine='python', sep=',', error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "unique_AnimeID = reviews['anime_uid'].unique()\n",
        "unique_users = reviews['profile'].unique()\n",
        "j = 0\n",
        "user_old2new_id_dict = dict()\n",
        "for u in unique_users:\n",
        "    user_old2new_id_dict[u] = j\n",
        "    j += 1\n",
        "j = 0\n",
        "movie_old2new_id_dict = dict()\n",
        "for i in unique_AnimeID:\n",
        "    movie_old2new_id_dict[i] = j\n",
        "    j += 1"
      ],
      "metadata": {
        "id": "eteVeXWWL6dh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
        "user_list = reviews['profile'].values\n",
        "movie_list = reviews['anime_uid'].values\n",
        "for j in range(len(reviews)):\n",
        "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
        "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
        "reviews['profile'] = user_list\n",
        "reviews['anime_uid'] = movie_list\n",
        "\n",
        "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
        "train_index = np.random.random(len(reviews)) <= 0.7\n",
        "train_df = reviews[train_index]\n",
        "test_df = reviews[~train_index]\n",
        "\n",
        "# generate train_mat and test_mat\n",
        "num_user = len(reviews['profile'].unique())\n",
        "num_items = len(reviews['anime_uid'].unique())\n",
        "\n",
        "train_mat = coo_matrix((train_df['score'].values, (train_df['profile'].values, train_df['anime_uid'].values)), shape=(num_user, num_items)).astype(float).toarray()\n",
        "test_mat = coo_matrix((test_df['score'].values, (test_df['profile'].values, test_df['anime_uid'].values)), shape=(num_user, num_items)).astype(float).toarray()\n",
        "train_data = torch.FloatTensor(train_mat)\n",
        "test_data = torch.FloatTensor(test_mat)"
      ],
      "metadata": {
        "id": "iyO3-NeU0Rey"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def predict_rating(user_index, item_index, ratings, item_similarity):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    k = 10\n",
        "    top_k = np.sort(arr, kind='quicksort')[::-1]\n",
        "    for i in range(ratings.shape[1]):\n",
        "        if i != item_index and ratings[user_index][i] != 0:\n",
        "            numerator += ratings[user_index][i] * item_similarity[item_index][i]\n",
        "            denominator += item_similarity[item_index][i]\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator / denominator\n",
        "\"\"\"\n",
        "\n",
        "def predict_rating(user_index, item_index, ratings, item_similarity, k=150):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    rated_items = np.where(ratings[user_index] != 0)[0]\n",
        "    #simm_items = np.argsort(item_similarity[item_index])[::-1][:k + 1].tolist()\n",
        "    simm_items = np.argsort(item_similarity[item_index])[::-1][:k].tolist()\n",
        "    for i in range(len(simm_items)):\n",
        "        if i != item_index  and ratings[user_index][i] != 0:\n",
        "            numerator += ratings[user_index][i] * item_similarity[item_index][i]\n",
        "            denominator += item_similarity[item_index][i]\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "35Hxqu64MAf9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "key_vit_50_dict = [x for x in vit_50_dict.keys()]\n",
        "embeddings = [[] for _ in range(num_items)]\n",
        "for i in key_vit_50_dict:\n",
        "  embeddings[movie_old2new_id_dict[i]] = vit_50_dict[i].reshape(-1) \n",
        "\n",
        "simm_item_ij = cosine_similarity(embeddings)"
      ],
      "metadata": {
        "id": "ENaMgrjZRYC2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in "
      ],
      "metadata": {
        "id": "0nR8Lup2otym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.count_nonzero(simm_item_ij > 0.7)\n",
        "print(count / simm_item_ij.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVb1TTMWbdY7",
        "outputId": "bf0f478a-6e01-4eb7-8b3a-1d227d74b1f4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8856713431686661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute predicted rating for a given user and item\n",
        "def predict_rating_based_users_who_rated_similar_items(item_idx, item_name, review_rating, simm_item_ij, k=10, user_column='profile', item_column='anime_uid'):\n",
        "    # Find k most similar items to the target item\n",
        "    sim_items = np.argsort(simm_item_ij[item_idx])[::-1][:k].tolist()\n",
        "    #print(sim_items)\n",
        "    #sim_items = simm_item_ij[item_idx].sort_values(ascending=False)[:k].index.tolist()\n",
        "    # Find all users who have rated those similar items\n",
        "    \n",
        "    sim_users = getattr(review_rating[getattr(review_rating,item_column).isin(sim_items)],user_column).tolist()\n",
        "    # Compute weighted average of their ratings for the target item\n",
        "    weights = [simm_item_ij[item_idx][sim_item] for sim_item in sim_items]\n",
        "    \n",
        "    #ratings_target = review_rating[(getattr(review_rating, user_column).isin(sim_users)) & (getattr(review_rating,item_column) == item_name)]\n",
        "    ratings_target = review_rating[(getattr(review_rating, user_column).isin(sim_users)) & (getattr(review_rating,item_column) == item_name)]\n",
        "\n",
        "    if len(ratings_target) == 0:\n",
        "        return 5\n",
        "    else:\n",
        "        print(np.array(weights).shape)\n",
        "        print(np.array(ratings_target.score).shape)\n",
        "        return np.average(ratings_target.score, weights=weights)"
      ],
      "metadata": {
        "id": "vFOiocIcdRM7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "sum_diff = 0\n",
        "indices_test = np.argwhere(test_mat > 0)\n",
        "cnt = len(indices_test)\n",
        "for index in indices_test:\n",
        "  predicted_rating = predict_rating(index[0], index[1], train_data, simm_item_ij, 2 50)\n",
        "  actual_rating = test_data[index[0]][index[1]]\n",
        "  diff = (actual_rating - predicted_rating)*(actual_rating - predicted_rating)\n",
        "  sum_diff = sum_diff + diff \n",
        "\n",
        "rmse = math.sqrt(sum_diff/cnt)\n",
        "print(\"RMSE = \", rmse)"
      ],
      "metadata": {
        "id": "wpAZqa5-Pccx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dde351-bfab-49f7-f981-9c3092b53823"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE =  5.508981653670089\n"
          ]
        }
      ]
    }
  ]
}