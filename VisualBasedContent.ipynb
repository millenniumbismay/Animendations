{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWf97cs2IBWo",
        "outputId": "94dcb538-6bb0-4f22-f460-192678f427d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Spring\\'23/CSCE670/Animendations/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTjRdgCtIHxz",
        "outputId": "41193506-cfcf-40c2-9bd9-a599ae1f5b50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Spring'23/CSCE670/Animendations\n",
            "'Animendations: Project Preview.gslides'\n",
            " data\n",
            " Gojo\n",
            " preprocessed_data\n",
            "'Project Proposal: Animendations.gdoc'\n",
            " synopsis_embeddings\n",
            " user_profile\n",
            " visual_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast"
      ],
      "metadata": {
        "id": "w_XlKkA5MYQP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animes_data_path = \"./preprocessed_data/new_preprocessed_animes.csv\"\n",
        "profiles_data_path = \"./preprocessed_data/preprocessed_profiles.csv\"\n",
        "reviews_data_path = \"./preprocessed_data/new_preprocessed_reviews.csv\"\n",
        "animes_data = pd.read_csv(animes_data_path)\n",
        "profiles_data = pd.read_csv(profiles_data_path)\n",
        "reviews_data = pd.read_csv(reviews_data_path, engine='python', sep=',', error_bad_lines=False)\n",
        "\n",
        "import pickle \n",
        "item_feature_dict = {}\n",
        "\n",
        "with open(\"./visual_embeddings/dict_1000_vit.pickle\", \"rb\") as f:\n",
        "  visual_embeddings_dict = pickle.load(f)\n",
        "with open(\"./synopsis_embeddings/sentence_embeddings_mpnet.pickle\", \"rb\") as f:\n",
        "  synopsis_embeddings_dict = pickle.load(f)\n",
        "\n",
        "q1 = reviews_data['anime_uid'].unique().tolist()\n",
        "q2 = [x for x in visual_embeddings_dict.keys()]\n",
        "q3 = [x for x in synopsis_embeddings_dict.keys()]\n",
        "\n",
        "s1 = set(q1)\n",
        "s2 = set(q2)\n",
        "s3 = set(q3)\n",
        "\n",
        "# s3 = s1-s2 \n",
        "s4 = s1 - s2.intersection(s3)\n",
        "l4 = list(s4)\n",
        "\n",
        "animes = animes_data.copy()\n",
        "profiles = profiles_data.copy()\n",
        "reviews = reviews_data.copy()\n",
        "for x in l4:\n",
        "  reviews.drop(reviews[reviews['anime_uid'] == x].index, inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-U_xuwqLm2s",
        "outputId": "5eef3913-69f2-4a81-a3b5-35cc90829805"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-bdcd6b59eacb>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  reviews_data = pd.read_csv(reviews_data_path, engine='python', sep=',', error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpteeL-WyD9E",
        "outputId": "3f01728b-db65-4606-b218-59adb3f472b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83918, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "unique_AnimeID = reviews['anime_uid'].unique()\n",
        "unique_users = reviews['profile'].unique()\n",
        "j = 0\n",
        "user_old2new_id_dict = dict()\n",
        "for u in unique_users:\n",
        "    user_old2new_id_dict[u] = j\n",
        "    j += 1\n",
        "j = 0\n",
        "movie_old2new_id_dict = dict()\n",
        "for i in unique_AnimeID:\n",
        "    movie_old2new_id_dict[i] = j\n",
        "    j += 1"
      ],
      "metadata": {
        "id": "eteVeXWWL6dh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
        "user_list = reviews['profile'].values\n",
        "movie_list = reviews['anime_uid'].values\n",
        "for j in range(len(reviews)):\n",
        "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
        "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
        "reviews['profile'] = user_list\n",
        "reviews['anime_uid'] = movie_list\n",
        "\n",
        "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
        "train_index = np.random.random(len(reviews)) <= 0.7\n",
        "train_df = reviews[train_index]\n",
        "test_df = reviews[~train_index]\n",
        "\n",
        "# generate train_mat and test_mat\n",
        "num_user = len(reviews['profile'].unique())\n",
        "num_items = len(reviews['anime_uid'].unique())\n",
        "\n",
        "train_mat = coo_matrix((train_df['score'].values, (train_df['profile'].values, train_df['anime_uid'].values)), shape=(num_user, num_items)).astype(float).toarray()\n",
        "test_mat = coo_matrix((test_df['score'].values, (test_df['profile'].values, test_df['anime_uid'].values)), shape=(num_user, num_items)).astype(float).toarray()\n",
        "train_data = torch.FloatTensor(train_mat)\n",
        "test_data = torch.FloatTensor(test_mat)"
      ],
      "metadata": {
        "id": "iyO3-NeU0Rey"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rating(user_index, item_index, ratings, item_similarity, k=150):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    rated_items = np.where(ratings[user_index] != 0)[0]\n",
        "    #simm_items = np.argsort(item_similarity[item_index])[::-1][:k + 1].tolist()\n",
        "    simm_items = np.argsort(item_similarity[item_index])[::-1][:k].tolist()\n",
        "    for i in range(len(simm_items)):\n",
        "        if i != item_index  and ratings[user_index][i] != 0:\n",
        "            numerator += ratings[user_index][i] * item_similarity[item_index][i]\n",
        "            denominator += item_similarity[item_index][i]\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "35Hxqu64MAf9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "item_feature_dict = dict()\n",
        "# synopsis_embeddings_dict = {k: v.detach().numpy() for k, v in synopsis_embeddings_dict.items()}\n",
        "\n",
        "all_anime_uid_list = list(s2.intersection(s3))\n",
        "item_feature_dict = {key: np.concatenate((visual_embeddings_dict[key][0], synopsis_embeddings_dict[key][0]), axis = 0) for key in all_anime_uid_list}\n",
        "\n",
        "embeddings = [[] for _ in range(num_items)]\n",
        "for i in item_feature_dict:\n",
        "  try:\n",
        "    embeddings[movie_old2new_id_dict[i]] = item_feature_dict[i].reshape(-1)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "simm_item_ij = cosine_similarity(embeddings)"
      ],
      "metadata": {
        "id": "ENaMgrjZRYC2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./visual_synopsis_item_item_sim.pickle', 'wb') as f:\n",
        "  pickle.dump(simm_item_ij, f)"
      ],
      "metadata": {
        "id": "6LuscFvc1erM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = np.count_nonzero(simm_item_ij > 0.7)\n",
        "print(count / simm_item_ij.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVb1TTMWbdY7",
        "outputId": "151bea54-3814-440a-921c-631bae9fc5f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4821444695363359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute predicted rating for a given user and item\n",
        "def predict_rating_based_users_who_rated_similar_items(item_idx, item_name, review_rating, simm_item_ij, k=10, user_column='profile', item_column='anime_uid'):\n",
        "    # Find k most similar items to the target item\n",
        "    sim_items = np.argsort(simm_item_ij[item_idx])[::-1][:k].tolist()\n",
        "    #print(sim_items)\n",
        "    #sim_items = simm_item_ij[item_idx].sort_values(ascending=False)[:k].index.tolist()\n",
        "    # Find all users who have rated those similar items\n",
        "    \n",
        "    sim_users = getattr(review_rating[getattr(review_rating,item_column).isin(sim_items)],user_column).tolist()\n",
        "    # Compute weighted average of their ratings for the target item\n",
        "    weights = [simm_item_ij[item_idx][sim_item] for sim_item in sim_items]\n",
        "    \n",
        "    #ratings_target = review_rating[(getattr(review_rating, user_column).isin(sim_users)) & (getattr(review_rating,item_column) == item_name)]\n",
        "    ratings_target = review_rating[(getattr(review_rating, user_column).isin(sim_users)) & (getattr(review_rating,item_column) == item_name)]\n",
        "\n",
        "    if len(ratings_target) == 0:\n",
        "        return 5\n",
        "    else:\n",
        "        print(np.array(weights).shape)\n",
        "        print(np.array(ratings_target.score).shape)\n",
        "        return np.average(ratings_target.score, weights=weights)"
      ],
      "metadata": {
        "id": "vFOiocIcdRM7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "sum_diff = 0\n",
        "indices_test = np.argwhere(test_mat > 0)\n",
        "cnt = len(indices_test)\n",
        "for index in indices_test:\n",
        "  predicted_rating = predict_rating(index[0], index[1], train_data, simm_item_ij, 250)\n",
        "  actual_rating = test_data[index[0]][index[1]]\n",
        "  diff = (actual_rating - predicted_rating)*(actual_rating - predicted_rating)\n",
        "  sum_diff = sum_diff + diff \n",
        "\n",
        "rmse = math.sqrt(sum_diff/cnt)\n",
        "print(\"RMSE = \", rmse)"
      ],
      "metadata": {
        "id": "wpAZqa5-Pccx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed80ad8-4157-4633-aa60-3063e7a7b49b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE =  5.479323100865358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Based content - compressed 50 vit - RMSE =  5.508981653670089\n",
        "### Visual Based content - 1000 vit - RMSE =  5.479323797063949\n",
        "### Synopsis Based content - 768 mpnet - RMSE =  5.494660646826089\n",
        "### Synopsis Based content - 50 mpnet - RMSE =  5.494175340940799\n",
        "### Visual + Synopsis Based content - 1000 vit + 768 mpnet - RMSE =  5.479323100865358"
      ],
      "metadata": {
        "id": "TajTLZF8YpAC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnIMgbBxLhr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}